# CKA Notes

## Certification 

- Certified Kubernetes Administrator
  - https://www.cncf.io/certification/cka/

- Exam Curriculum (Topics) 
  - https://github.com/cncf/curriculum

- Candidate Handbook 
  - https://www.cncf.io/certification/candidate-handbook

- Exam Tips
  - http://training.linuxfoundation.org/go//Important-Tips-CKA-CKAD


## Cluster Architecture 

### K8s Architecture
- Master 
  - Manage 
  - Plan
  - Schedule 
  - Monitor nodes 
- Worker Nodes
  - Host application as Containers 
  - Minions 
	
- Master Side 	
  - ETCD Cluster 
    - DB its stores information in key value format of cluster data 
  - Kube-Scheduler 
    - Watches for newly created pods with no assigned node, and selects a node for them to run on.
  - Kube Controller Manager 
    - Control Plane component that runs controller processes.
    - Node Controller 
	  - Responsible for noticing and responding when nodes go down.
    - Replication Controller
	  - Responsible for maintaining the correct number of pods for every replication controller object in the system
	- Endpoints Controller 
	  - Populates the Endpoints object (that is, joins Services & Pods).
    - Service Account & Token Controllers
      - Create default accounts and API access tokens for new namespaces.
  - Kube-Api server 
    - For communication 
	- Kubernetes control plane that exposes the Kubernetes API. The API server is the front end for the Kubernetes control plane.
	
- Worker Side 	
  - Container Runtime Engine 
    - software that is responsible for running containers.
    - Docker 
	- Rocket (rkt)
	- Containerd
  - Kubelet
    - Agents runs on the node 
	- It makes sure that containers are running in a pod.
  - Kube-Proxy 
    - Communication between nodes
    - network proxy that runs on each node in your cluster, implementing part of the Kubernetes Service concept.	
	
- Addons 
  - DNS
    - Cluster DNS is a DNS server, in addition to the other DNS server(s) in your environment, which serves DNS records for Kubernetes services.
  - WebUI
    - Dashboard 
	- It allows users to manage and troubleshoot applications running in the cluster, as well as the cluster itself.
  - Container Resource Monitoring
    - Container Resource Monitoring records generic time-series metrics about containers in a central database, and provides a UI for browsing that data.
  - Cluster-level Logging
    - A cluster-level logging mechanism is responsible for saving container logs to a central log store with search/browsing interface.

- Pod 
  - A Pod is the basic execution unit of a Kubernetes application
  - the smallest and simplest unit in the Kubernetes object model that you create or deploy. 
  - A Pod represents processes running on your Cluster.
  - Each Pod is assigned a unique IP address. 
  - Every container in a Pod shares the network namespace, including the IP address and network ports. 
  - Containers inside a Pod can communicate with one another using localhost. 
  - A Pod can specify a set of shared storage Volumes. 
  - All containers in the Pod can access the shared volumes, allowing those containers to share data.
  - Pods that run a single container
    - one to one 
	- most common use case 
	- one container per pod
  - Pods that run multiple containers that need to work together.
    - A Pod might encapsulate an application composed of multiple co-located containers that are tightly coupled and need to share resources.

- Yaml
  - *.yaml or *.yml 
  - apiVersion
  - kind 
    - Pod  (apiversion: v1)
	- Namespace  (apiversion: v1)
	- ResourceQuoto (apiversion: v1)
	- Service (apiversion: v1)
	- ReplicaSet (apiversion: apps/v1)
	- RelicationController (apiversion: v1)
	- Deployment (apiversion: apps/v1)
  - metadata 
    - name
	- labels 
  - spec 
    - containers (array) 
	  - name 
	  - image 
	  
- RelicaSet
  - purpose is to maintain a stable set of replica Pods running at any given time
  - Load balancing and scaling 
  - In yaml 
  - spec
    - replicas 
	- selector
	  - matchLabeles 
	- template 
	  - embed the pod configuration 
	
- RelicationController
  - ensures that a specified number of pod replicas are running at any one time.
  - In yaml 
  - spec
    - replicas 
	- selector
	- template 
	  - embed the pod configuration 
  - difference between replica set and replication controller 
    - both are doing the same job
	- Replica Set use Set-Based selectors while replication controllers use Equity-Based selectors.
	- The replication controller makes sure that few pre-defined pods always exist. So in case of a pod crashes, the replication controller replaces it.
	- Replica sets are comparatively more useful. In recent times replication sets have replaced replication controllers.
	- supports matching labels
	
- Deployments
  - provides declarative updates for Pods and ReplicaSets.
	
- Namespaces 
  - default 
    - The default namespace for objects with no other namespace
  - kube-system 
    - The namespace for objects created by the Kubernetes system
  - kube-public 
    - This namespace is created automatically and is readable by all users
	- This namespace is mostly reserved for cluster usage
  - kubectl config set-context --current --namespace=<insert-namespace-name-here>
  - To validate 
    - kubectl config view --minify | grep namespace:
  - When you create a Service, it creates a corresponding DNS entry
    - service-name>.<namespace-name>.svc.cluster.local
	
- ResourceQuoto 
  - provides constraints that limit aggregate resource consumption per namespace
  
## Configuration 

- Commands 
  - JSON format
  - command - overrides  ENTRYPOINT in docker file
  - args - overrides CMD in docker file 

- ENV value type 
  - Environment variable 
  - key value pair 
  - spec section 
  - array 
  - env 
    - name 
	- value
	
	- valueFrom 
	  - configMapKeyRef
	  - secretKeyRef

	  
- Config Maps
  - allow you to decouple configuration artifacts from image content to keep containerized applications portable	  
  - 2 phases 
    - 1. creating config map 
	- 2. inject into the pod 
  - 2 ways of creating config map
  - 1. Imperative way 
    - kubectl create configmap <configname> <data-source>
	- kubectl create configmap <configname> --from-lieral=<key>=<value> 
	- kubectl create configmap <configname> --from-file=<path-to-file> 
  - 2. Declarative way
    - kubectl create -f map.yaml
	- apiVersion: v1
	- kind: ConfigMap
	- metadata:
	  -  name: app-config 
	- data:
	  - <data>
	- data tag instead of spec
  - 2 ways of injecting into pod 
    - 1. inject specific env value from config map to pod 
	- using valueFrom tag
	  - env 
		- name: <env_name> 
		- valueFrom: 
		  - configMapKeyRef: 
		    - name: <config-map_name)
			- key: <key from config map)
	- 2. inject all the key pair values from configmap into pod 
	- using envFrom tag 
	- envFrom 
      - configMapRef
	    - name: <configmap-name>
	 
- Secrets 
  - same as configmaps
  - this is to store confidential data 
  - creating secrets in imperative way   
    - kubectl create secret generic <secret-name> <data-source>
	- kubectl create secret generic <secret-name> --from-lieral=<key>=<value> 
	- kubectl create secret generic <secret-name> --from-file=<path-to-file> 
  - creating secrets in declarative way   
    - kubectl create -f secrets.yaml
	- apiVersion: v1
	- kind: Secret
	- metadata:
	  -  name: app-config 
	- data:
	  - <data>
	- data tag instead of spec
	- for Yaml data should be base64 encrypted 
	  - echo -n 'mysql' | base64
    - describe secrets will show only the key namespace
	- get secrets will show the encrypted value of key pair
	- To decrypt the values 
	  - echo -n 'bahbfdh=' | base64 --decode 
   - Secrets can be used As files in a volume mounted on one or more of its containers
	  
- Security Context
  - Docker Security 
    - containers and hosts shares same kernal 
	- its isolated by namespaces
	- docker container has its own namespace 
	- it cannot see outside of its namespace
	- with in the container if you list the
	  processes with ps aux command it list the 
	  processes running with in the container 
	- same ps aux command at the lost level also
	  list the same processes but with different 
	  process id
	- process isolation
	- by default docker runs processes as root user 
	- if you want enforce different user use --user command 
	- or USER in dockerfile 
	- docket limits the abilities of root user with in the container 
	- root user in the host and root user in container are not same
	  in terms of capability 
	- by default docker limits the capabilities of root user in 
	  container 
	- say for example you cant reboot the host 
	- to override 
	  - --cap-add MAC_ADMIN (to add permissions)
	  - --cap-drop KILL (to drop)
	  - for all the premissions use --privileged option 
  - Container Security 
    - Security Context 
	- can set the security at pod level or container level
	- if you set at pod level its carried to all the containers
	- if you set at container level it overrides the pod security 
	- in pod yaml 
	  - securityContext:
	    - runsAsUser: 1000
		- capabilities:
		  - add: ["MAC_ADMIN"]
    - to set at pod level have the securityContext under spec
	- to set at container level have the securityContext under containers tag
	
- Service Account
  - A service account provides an identity for processes that run in a Pod.
  - role based access controls (out of scope for CKA)
  - authentication and authorization 
  - 2 types of accounts
    - 1. user account 
	  - Developer accessing cluster to do the deployment
    - 2. Service account 
      - used by application to interact with k8s cluster
      - promotheous monitoring application
      - jenkins to do the deployment	  
  - kubectl create serviceaccount <name>
  - it creates the token 
  - token is stored as secret object 
  - to view secret --> describe secret 
  - Bearer token 
  - if the third party hosted in the same cluster 
    - automatically the mounting the secret token with in the pod as volume pod 
  - for every namespace has default service account 
  - by default will be using default service account (describe pod will show)
  - default service account is restricted supports only basic api queries 
  - should update the pod for new service account 
  - delete and recreate the pod
  - to  opt out of automounting API credentials for a pod, set automountServiceAccountToken to false in spec section
  - to not to mount the default secrete volume use automountServiceAccountToken attribute in spec
  
- Resource Requirements 
  - CPU
  - MEM
  - DISK
  - k8s scheduler places the pods into nodes based on pod and node capacity
  - if scheduler couldn't fit the pod into any node, pod will be in waiting state and Events it will show Insufficient cpu
  - by default k8s assumes pod or container require 0.5 CPU and 256 Mi MEM
  - By default k8s scheduler uses this number to identify the available node 
    - for defaults set LimitRange (kind: limitRange)
  - to overrides the default values in spce section of pod 
    - resources: 
	  - requests: 
	    - memory: "1Gi"
		- cpu: 1
  - cpu can be from 0.1 cpu
  - 0.1 is also represented as 100m 
  - m stands for milli
  - can go low as 1m, but less then that 
  - 1 CPU is equal to 
    - 1 AWS vCPU
	- 1 GCP Core
	- 1 Azure Core 
	- 1 Hyperthread 
  - with memory 
    - 256 Mi (mebi bytes)
	- or 268435456
	- 1G or 1M or 1K
	- 1Gi or 1Mi or 1 Ki
	- 1 Ki - 1 Kibibyte - 1024 bytes 
	- 1 Kb - 1 Kilobyte - 1000 bytes 
  - in docker world container has no limit to consume resource
  - lets say if container starts with 1 cpu and it can consume max available cpu of the node 
  - in k8s can limit this using limits section in resoucres
    - resources: 
	  - requests: 
	    - memory: "1Gi"
		- cpu: 1
	  - limits:
	    - memory: "2Gi"
		- cpu: 2
   - in case if resource exceeds its limit   
     - for cpu, k8s throttles 
	 - it cant use beyond the limit defined 
   - but with memory 
     - container can use more memory then limit 
	 - if it constantly uses more memory then it will be terminated
	 
- Taints and Tolerations
  - to set restrictions on what pod can be scheduled to what node 
  - no guaranteed to place pod on a particular node (Node affinity)
  - to ensure that pods are not scheduled onto inappropriate nodes. 
  - One or more taints are applied to a node
  - this marks that the node should not accept any pods that do not tolerate the taints
  - Tolerations are applied to pods, and allow (but do not require) the pods to schedule onto nodes with matching taints.
  - By default pods have no tolerations 
  - Opposite to Node affinity 
  - to add taint to node 
    - kubectl taint nodes node1 app=blue:NoSchedule
  - to remove taint from node 
    - kubectl taint nodes node1 app:NoSchedule-
  - to add toleration to pod, under spec 
    - tolerations:
	  - - key: "key"
        - operator: "Exists"
        - effect: "NoSchedule"  
	  - - key: "key"
		- operator: "Equal"
		- value: "value"
		- effect: "NoExecute"
		- tolerationSeconds: 3600
  - values are in double quotos 
  - its array, so can more then 1 tolerations
  - the operator is Exists (in which case no value should be specified)
  - the operator is Equal and the values are equal
  - effect, 3 types 
    - 1. NoSchedule
	  - new Pods will not be scheduled on the node 
	  - no effect of existing pods
	- 2. NoExecute
	  - any existing pods that do not tolerate the taint will be evicted immediately
	  -  pods that do tolerate the taint will never be evicted
	  - tolerationSeconds - dictates how long the pod will stay bound to the node after the taint is added.
	    - optional property 
	- 3. PreferNoSchedule
	  - “soft” version of NoSchedule 
	  – the system will try to avoid placing a pod that does not tolerate the taint on the node, but it is not guaranteed 
  - By default Master node is tainted automatically to avoid accepting the pods 
  - this can be changed and but recommended not to host the pods in master node 
  - to see this configuration 
    - kubectl describe node kubemaster | grep Taint
	
- Node Selector 
  - to limit a pod to place on a particular node 
  - for simple selection ( “AND or exact match”)
  - use label selectors to make the selection. 
  - in Pod 
    - spec section add 
	  - nodeSelector:
		- disktype: large 
    - in Node add this label and selectors 
	  - kubectl label nodes Node1 disktype=ssd
  
- Node Affinity 
  - it allows you to constrain which nodes your pod is eligible to be scheduled on, based on labels on the node
  - advanced expression 
    - OR
	- XOR
  - can add soft rule.  if the scheduler can’t satisfy it, the pod will still be scheduled
  - two types of affinity, 
    - 1. node affinity 
	- 2. inter-pod affinity or anti-affinity
  - two types of node affinity
    1. requiredDuringSchedulingIgnoredDuringExecution  (hard)
	2. preferredDuringSchedulingIgnoredDuringExecution  (soft)
	3. (planned) requiredDuringSchedulingRequiredDuringExecution
  - can be added to pod in spec section 
    - affinity:
      - nodeAffinity:
        - requiredDuringSchedulingIgnoredDuringExecution:
          - nodeSelectorTerms:
          - - matchExpressions:
            - - key: kubernetes.io/e2e-az-name
              - operator: In
              - values:
              - - e2e-az1
              - - e2e-az2
        - preferredDuringSchedulingIgnoredDuringExecution:
        - - weight: 1
          - preference:
            - matchExpressions:
            - - key: another-node-label-key
              - operator: In
              - values:
              - - another-node-label-value
    - operator In represents OR conditions.   
	- operator NotIn represents XOR conditions.   
	- operator Exists check whether the label presents with node  (value is not required)

- Multi Container Pods 
  - in pod definition, spec section, container is array. 
  - means pod can have more then 1 containers 
  - multi container pods shares the same 
    - lifecycle (scale up and down together)
	- network (can be accessed through localhost)
	- storage (volume sharing is not required)
  - patterns 
    - Sidecar 
	  - login service along with application 
    - Ambassador 
	  - you have 3 DB dev, stage and prod.  application always contacts ambassdor container as localhost, in which it has logic to divert to respective environment 
    - Adaptor 
	  - different applications produces logs in different format.  Adaptor container processes the logs to standard format and sends to centralized logging server 
	  
## Observability 
	  
- POD Status
  - Pending 
  - ContainerCreating 
  - Running 
  
- POD Conditions 
  - array of true or false 
  - PodScheduled 
  - Initialized 
  - ContainersReady 
  - Ready 
    - once the condition is set to ready, service will send the traffic to the pod 
	- By default pod is set to be ready once the container started 
	- it didnt check the status of underlying app 

	  
- Liveness probe
  - To know when to restart a container. 
  - For example, liveness probes could catch a deadlock, where an application is running, but unable to make progress. 
  - Restarting a container in such a state can help to make the application more available despite bugs.	
  - in spec section 
    - livenessProbe: 
      -	initialDelaySeconds: 5 # should wait 5 second before performing the first probe
      - periodSeconds: 5 # specifies that the kubelet should perform a liveness probe every 5 seconds
	  - failureThreshold: 1
	  - successThreshold: 
	    - Minimum consecutive successes for the probe to be considered successful after having failed. 
		- Defaults to 1. 
		- Must be 1 for liveness. 
		- Minimum value is 1.
      - failureThreshold: 
	    - When a Pod starts and the probe fails, Kubernetes will try failureThreshold times before giving up. 
		- Giving up in case of liveness probe means restarting the container. 
		- In case of readiness probe the Pod will be marked Unready. 
		- Defaults to 3. 
		- Minimum value is 1.
  - probes are 
    - exec 
	  - executes the command
	  - If the command succeeds, it returns 0, and the kubelet considers the container to be alive and healthy. 
	  - If the command returns a non-zero value, the kubelet kills the container and restarts it.
	- httpGet
	  - path: /healthz
        port: 8080
        httpHeaders:
        - name: Custom-Header
          value: Awesome
	  - Any code greater than or equal to 200 and less than 400 indicates success. Any other code indicates failure.
	- tcpSocket
	  - port: 8080

- Readiness Probe 
  - To know when a container is ready to start accepting traffic
  - A Pod is considered ready when all of its containers are ready.
  - By default pod is set to be ready once the container started 
  - Readiness probes runs on the container during its whole lifecycle.
  - same as liveness probe 
  - in spec section add 
    - readinessProbe: 
  
- Container Logging 
  - To view the container logs 
    - kubectl logs -f <pod-name>
  - In case if its multi container pod 
    - kubectl logs -f <pod-name> <container-name>
  
- Monitoring K8s
  - Prometheus 
  - Elastic Stack
  - DataDog
  - Dynatrace
  - Metrics Server
    - Heapster is deprecated 
	- can have 1 metrics server per k8s cluster 
	- In Memory monitoring solution 
	- doesn't store the results in disk 
	- so you cant see historical analytics 
	- Kubelet agent also contains cAdvisor component 
	- container advisor 
	  - responsible for collecting the metrics and sends to metrics server using kube api's 
	- For minikube 
	  - minikube addons enable metrics-server 
	- for others 
      - clone from github and install 
	- kubectl top nodes 
	- kubectl top pod
  
## POD Design 

- Labels and Selectors 
  - Labels are key/value pairs that are attached to objects, such as pods.
  - in Pods, under metadata section
  - metadata:
      - name: label-demo
      - labels:
          - environment: production
          - app: nginx
  - labels do not provide uniqueness. In general, we expect many objects to carry the same label(s).
  - Via a label selector, the client/user can identify a set of objects. 
  - The label selector is the core grouping primitive in Kubernetes.
  - equality based selector
    - kubectl get pods -l environment=production,tier=frontend
  - set based selector
    - kubectl get pods -l 'environment in (production),tier in (frontend)'
  - In service or Replication Controller 
    - equality based selectors are supported
    - The set of pods that a service targets is defined with a label selector. 
	- selector:
      - component: redis
  - In Job, Deployment, ReplicaSet, DaemonSet
    - support set-based selectors as well.
	- selector:
		- matchLabels:
			- component: redis
		- matchExpressions:
			- - {key: tier, operator: In, values: [cache]}
			- - {key: environment, operator: NotIn, values: [dev]}

- Updates and Rollbacks in Deployment
  - Deployment automatically creates replicaset with replicas defined 
  - Deployment triggers the rollout which is revision 1
  - when deployment is updates new rollout will be triggered and revision will be 2
  - to track the changes of the previous deployments, stores the configuration for rollback
  - after the upgrade 
	- kubectl get replicasets
	- shows both old and new replicaset 
  - to view the rollout of the deployment
    - kubectl rollout status <deployment-name>
  - To view the history 
    - kubectl rollout history <deployment-name>
  - To check the revisions details individually 
    - kubectl rollout history deployment <deployment-name> --revision=<revision-no>
  - By default change cause in the history will be none. 
  - To start recording the change cause in deployments
    - --record flag 
	- kubectl apply -f  <deployment.yml> --record
  - 2 types of Deployment Strategy 
    - Details can be seen with describe deployment command 
    - 1. Recreate 
	  - destroys all the pods at once and create new set of pods 
	  - outage
	  - not default 
	  - .spec.strategy.type==Recreate
	- 2. RollingUpdate 
	  - creates another replica set 
	  - destroys 1 pod in old replica set and create 1 pod in new 
	  - no outage
	  - .spec.strategy.type==RollingUpdate
	  - default deployment strategy 
  - To update the deployment
    - kubectl apply -f <deployment.yml>
  - To update the image alone 
    - kubectl set image <deployment-name> nginx=nginx:1.9.1
  - For Rollback 
    - kubectl rollout undo <deployment-name>
	- reverts to previous configuration of the deployment
	- can see the difference by listing replicasets 

- Jobs 
  - For batch processing 
  - A Job creates one or more Pods and ensures that a specified number of them successfully terminate. 
  - As pods successfully complete, the Job tracks the successful completions. 
  - When a specified number of successful completions is reached, the task (ie, Job) is complete. 
  - Deleting a Job will clean up the Pods it created.
  - definition
    - apiVersion: batch/v1
    - kind: Job
	- in spec: 
	  - add template 
	  - restartPolicy: Never
  - Lets say Pod is created with image which adds two numbers prints the output end 
  - When the pod is created it does the addition task and ends the process 
  - by default Pod tries to recreate the container again and it goes till it meets the restart thershold 
  - by default k8s wants to keep your pod running for ever. so it restarts to make it live (Job usecase)
  - To avoid this add restartPolicy: Never in spec of pod.yaml
  - default policy is Always 
  - To View the Jobs 
    - kubectl get jobs 
  - Job creates Pods
  - To view the output of the job, check the logs of the pod created 
    - kubectl logs <pod-name>
  - to delete job 
    - kubectl delete job <job-name>
	- deleting the job also deletes the pods created 
  - To run multiple instance of pods to run in job
    - add completions in spec section 
	  - completions: 3
	  - it creates 3 pods sequentially by default 
      - 1 after another 
      - second pod is created only after first pod is finished 
	  - in case if one of the pod is failed, k8s continue creates pod till it reach the completions count 
  - To create the pods in parallel 
    - in spec section add parallelism
	  - completions: 3
	  - parallelism: 3
  - backoffLimit
    - There are situations where you want to fail a Job after some amount of retries due to a logical error in configuration
	- in spec section of job 
	  - backoffLimit: 15  # retries creating pod 15 times till it success
	  - default is 6 
	  - if the backoffLimit is not mentioned, Job retries 6 times. it creates the pod till success status 
	  
- Cron Jobs:
  - A Cron Job creates Jobs on a time-based schedule.
  - One CronJob object is like one line of a crontab (cron table) file. 
  - It runs a job periodically on a given schedule, written in Cron format.
  - Job runs instantly 
  - Instead we can create Cron Job and schedule the job periodically 
  - definition 
    - apiVersion: batch/v1beta1
    - kind: CronJob
    - metadata:
        - name: hello
    - spec:
      - schedule: "*/1 * * * *"
      - jobTemplate:
	    - spec section of Job 
  - it has 3 spec sections 
    - 1 is for cronjob 
	- 2 is for Job 
	- 3 is for Pod
  - To View the cron jobs 
    - kubectl get cronjob
	
# Services and Networking
  
- Service 
  - An abstract way to expose an application running on a set of Pods as a network service.
  - Types
    - 1. Nodeport
	- 2. ClusterIp
	- 3. LoadBalancer
  - NodePort
    - Exposes the Service on each Node’s IP at a static port (the NodePort).
	- A ClusterIP Service, to which the NodePort Service routes, is automatically created. 
	- You’ll be able to contact the NodePort Service, from outside the cluster, by requesting <NodeIP>:<NodePort>.
	- 3 ports 
	  - Target port, port of the pod 
	  - port, port of the service 
	  - NodePort 
	    - 30000 to 32769 range
    - in spec section
	  - type: NodePort 
	  - ports:
	  - selector:
	- ports is array, can add more then one set of ports mapping
	- port is mandatory 
	- if target port is not given, port will be taken as target port 
	- if nodeport is not given, available port from the range will be taken
	- if selector matches more then one pod, service distribute the traffic to all the pods
	  - no additional configuration required 
	  - uses random algorithm 
	  - SessionAffinity is Yes
	  - in case Pod is spread across multiple Pods, 
	    - k8s spans the service across the nodes automatically
		- service can be accesses via any NodeIp and nodeport 
		- Nodeport remains same 
  - ClusterIp  
    - This is the default ServiceType.
	- Exposes the Service on a cluster-internal IP. 
	- Choosing this value makes the Service only reachable from within the cluster. 
	- creates virtual IP inside cluster to enable the communication within the cluster
	- service can be accessed either by cluster ip or service name 
  - LoadBalancer
    - Exposes the Service externally using a cloud provider’s load balancer. 
	- NodePort and ClusterIP Services, to which the external load balancer routes, are automatically created.
	- sends request to cloud platform to provision network load balancer 
	
- Ingress
  - An API object that manages external access to the services in a cluster, 
  - provides 
    - load balancing, 
	- SSL termination 
	- name-based virtual hosting
	- url based routing 
  - Ingress --> Service --> Deployment --> Pod
  - Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. 
  - Traffic routing is controlled by rules defined on the Ingress resource.
  - built in layer 7 load balancer (like)
  - uses a service of type Service.Type=NodePort or Service.Type=LoadBalancer to expose
  - Ingress Controller 
    - responsible for fulfilling the Ingress, 
	- k8s doesn't come with ingress controller by default 
    - solutions
	  - GCE (Googlr layer 7 load balancer)
	  - Nginx
	  - Contour
	  - HAProxy 
	  - Traefik
	  - Istio
	- GCE and Nginx are currently supported by k8s project
	- controller is deployed as just anohter k8s deployment 
	- Nginx Controller 
	  - Deployment yaml 
	  - ConfigMap
	  - Service
	  - ServiceAccount 
  - Ingress Resources  
    - Set of rules and configurations applied on the ingress controller 
    - You must have an ingress controller to satisfy an Ingress. 
	- Only creating an Ingress resource has no effect.
	- yaml
	  - apiVersion: extensions/v1beta1
	  - kind: Ingress 
	  - spec
	    - backend:
		    - serviceName:
			- servicePort 
	- To view
	  - k get ingress 
	- Rules 
	  - host
	    - optional 
		- if no host is specified, so the rule applies to all inbound HTTP traffic through the IP address specified. 
		- If a host is provided, the rules apply to that host.
	  - path
	    -  list of paths (for example, /testpath), 
		- each of which has an associated backend defined with a serviceName and servicePort. 
		- Both the host and path must match the content of an incoming request before the load balancer directs traffic to the referenced Service.
    - default back-end 
	  - if the rules doesn't satisfy default will be routed
	  - must create the default service to show 404 error page (eg)
  - Rewrite annotations
    - https://kubernetes.github.io/ingress-nginx/examples/rewrite/  

- Network Policies 
  - Ingress - incoming traffic 
  - Egress - outgoing traffic
  - A network policy is a specification of how groups of pods are allowed to communicate with each other and other network endpoints.
  - NetworkPolicy resources use labels to select pods and define rules which specify what traffic is allowed to the selected pods.
  - by default k8s follow all-allow rule 
  - all the pods in cluster is connected 
  - By default, pods are non-isolated; they accept traffic from any source.
  - network policy is used to isolate the pod 
  - Yaml
    - apiVersion: networking.k8s.io/v1
    - kind: NetworkPolicy
	- in spec:
	  - podSelector: 
	    - Each NetworkPolicy includes a podSelector which selects the grouping of pods to which the policy applies.
		- An empty podSelector selects all pods in the namespace.
      - policyTypes: 
	    - Each NetworkPolicy includes a policyTypes
     	  - Ingress
		  - Egress
		  - or both. 
		- The policyTypes field indicates whether or not the given policy applies to ingress traffic to selected pod, egress traffic from selected pods, or both. - If no policyTypes are specified on a NetworkPolicy then by default Ingress will always be set and Egress will be set if the NetworkPolicy has any egress rules.
	  - ingress: from (or) egress: to
	    - podSelector
		  - This selects particular Pods in the same namespace as the NetworkPolicy which should be allowed as ingress sources or egress destinations.
		- namespaceSelector: 
		  - This selects particular namespaces for which all Pods should be allowed as ingress sources or egress destinations.
  - solutions supports networking policies 
    - Kube-router
	- Calico
	- Romana
	- weave-net
  - solutions supports networking policies 
    - Flannel 

## State Persistence 

- Volumes
  - On-disk files in a Container are ephemeral
  - when container crashes or stopped data will be lost 
  - Volumes 
    - to store and manage the data
	- To share files between the containers 
  - in POD definitions, spec sections
    - 1. create volumes 
	  - volumes:
	    - name: test-volume
	    - hostpath: 
	      - path: /data 
		  - type: directory 
    - 2. mount the volume in container section 
	  - volumeMounts:
        - mountPath: /test-pd
        - name: test-volume
  - for multi pods, k8s supports many storage solution 
    - NFS 
	- AWS EBS etc 
		
- Persistent Volume
  - PV
  - Piece of storage in the cluster that has been provisioned by an administrator
  - Volumes are configured within the pod
  - for large environment, centralized volume solution is persistent volume
  - administrator provisions large pool of storage is persistent volume
  - users can select the storage from the pool is PersistentVolumeClaim
  - yaml 
    - apiVersion: v1
	- kind: PersistentVolume
	- spec 
	  - accessMode:
	    - ReadOnlyMany 
		- ReadWriteOnce
		- ReadWriteMany
	  - capacity:
	    - storage: 1Gi
	  - hostPath:
	    - path: /tmp 
	  - for aws, replace the hostPath options with 
        - awsElasticBlockStore
          - volumeID:
          - fstType: ext4		  
  
- PersistentVolumeClaim
  - PVC
  - (PVC) is a request for storage by a user
  - to make the storage available to node 
  - Admin creates set of PV
  - user creates set of PVC
  - once the PVC created k8s binds the PVC with respective PV based on size and accessMode
  - PV to PVC is one to one
  - in case if multiple matches use label and selectors to bind to PV
  - PVC couldn't find PV, it will be in pending state 
  - if small PVC binded to large PV (if no oher better option), since its one to one relation, no other PVC can utilise the remaining capacity of PV
  - Yaml
    - apiVersion: v1
	- kind: PersistentVolumeClaim
	- spec 
	  - accessMode:
	   - ReadWriteMany
	  - capacity:
	    - storage: 500Mi
  - to view 
    - k get pvc
  - to delete pvc
    - k delete pvc <pvc-name>
	- what happens the underlying pv when pvc is deleted 
	  - persistentVolumeClaimPolicy 
	    - Retain 
		  - by default
		  - PV will remains untill administrator delete it manually 
		  - its not available for re use for any other claims 
		- Delete 
		  - when PVC is deleted PV also deleted automatically
		  - freeing up the memory 
		- Recycle 
		  - Data will be scrubbed and make it available for other PVC;same
		  - deprecated
		  - use dynamic provisioning
  - Cliams as Volumes
    - once the PVC is created use it in volumes section of the pod
    - mounting remains same 
    - volumes:
      - name: mypd
      - persistentVolumeClaim:
          - claimName: myclaim	
		  
- Storage Classes
  - Dynamically provisioning the storage
  - admin no need to create PV and provision the storage manually (static provisioning) 
  - in PVC use the storage class name, 
  - cloud provider dynamically provision the storage as per PVC 
  - K8s automatically created PV and bind behind the screen 

- StatefulSet  
  - StatefulSet is the workload API object used to manage stateful applications.
  - Same as Deployment
  - kind: StatefulSet
  - Stable, unique network identifiers.
  - Stable, persistent storage.
  - Ordered, graceful deployment and scaling.
  - Ordered, automated rolling updates.
  - StatefulSet maintains a sticky identity for each of their Pods. 
  - These pods are created from the same spec, but are not interchangeable: 
  - each has a persistent identifier that it maintains across any rescheduling.
  - if the replica is set to 3, 3 pods are crated sequentially one after another 
  - pod name is suffixed in order from 0 to 1 
  - pod-0, pod-1 and pod-2 
    - in case of scale up, pod 3 wil be created 
	- in case of scale down pod 2 will be termineted 
	- in case pod-o is crashed, StatefullSet provisions another new pod with the same name pod-0 
	- sticky session
  - MySql DB replication use case 
  - Parallel pod management 
    - Parallel tells the StatefulSet controller to launch or terminate all Pods in parallel
	- by default its sequential
	
- Generators 
  - Pod 
    - run-pod/v1
	- restart=Never
  - Replication Controller 
    - run/v1
  - Deployment 
    - deployment/v1beta1
	- Default one 
  - Job 
    - job/v1
	- restart=OnFailure
  - CronJob 
    - cronjob/v1beta1
	- restart=OnFailure --schedule=<some cron expression>
	
  
		
   
- Vim Setup
  - vi ~/.vimrc
  - set ts=2 sts=2 sw=2 et number
  - Save and exit and then reload the file: . /~.vimrc
  
- Vim Tips 
  - 'dG' - Deletes contents from cursor to end of file. This is very useful when editing YAML files.
  - 'ZZ' - Save and exit quickly.
	  
	  
	  
    
  
  
  
- Commands 
  - kubectl run hello-world 
  - kubectl cluster-info 
  - kubectl get nodes 
  - kubectl create -f
  - kubectl apply -f 
  - kubectl edit pods
  - kubectl replace -f 
  - kubectl scale --replicas=5 replicaset new-replica-set
  - kubectl get all
  - kubectl get pods --all-namespaces
  - kubectl run redis --image=redis123 --generator=run-pod/v1 -l app=run  or k run mosquito --image=nginx --restart=Never
  - kubectl get pod my-pod -o yaml --export       # Get a pod's YAML without cluster specific information
  - kubectl delete pods <pod> --grace-period=0 --force
  - echo -n 'mysql' | base64
  - echo -n 'bahbfdh=' | base64 --decode 
  - kubectl exec ubuntu-sleeper whoami  # To check the user 
  - kubectl exec -it app cat /log/app.log
  - kubectl exec -it <pod-name> -c <container-nmae> <command> # for multi containers 
  - kubectl get nodes node01 --show-labels
  - kubectl get pods -l environment=production,tier=frontend
  - kubectl get pods -l 'environment in (production),tier in (frontend)'
  - kubectl expose deployment -n ingress-space ingress-controller --type=NodePort --port=80 --name=ingress --dry-run -o yaml >ingress.yaml
  - k run b3 --image=busybox --restart=Never --rm  -it env   # executes env command and deletes the pod 
  - kubectl run busybox --image=busybox --labels=c=d --restart=Never --rm -it -- wget -O- http://10.109.97.129:80 # executes wget command and deletes the pod 
  - k exec nginx -- /bin/sh
  - k run b --image=busybox --restart=Never --dry-run -o yaml -- /bin/sh -c 'i=0; while true; do echo "$i: $(date)"; i=$((i+1)); sleep 1; done' >b.yml #Shell arguments 
  - kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10 # AutoScale
  - kubectl get hpa # Horizantal Pod AutoScaler
  
- Clear
  - https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims
    - array

  


  
	
